{"date": "2023-06-01", "title": "Bitcoin Optech: Newsletter #253 Recap]", "url": "https://anchor.fm/s/d9918154/podcast/play/71718216/https://d3ctxlq1ktw2nl.cloudfront.net/staging/2023-5-6/3101e9c3-4032-d23b-659c-d297dde80d9d.mp3", "body": "\ufeff1\n00:00:00,060 --> 00:00:48,599\nSpeaker 0: Welcome everybody to Bitcoin OpTech Newsletter number 253. It is Thursday, June 1st and we are doing a Twitter space to recap this newsletter. We'll be talking about the ARK protocol. We'll also be talking about transaction relay over a NOSTR. We'll be talking about bidding for block space in our series about mempool inclusion and transaction selection. We have Q&A from the Stack Exchange. We have Bitcoin Core 25.0 being released. And then we have a few notable PRs that we also covered in the newsletter this week. So let's jump into it. We'll do introductions. I'm Mike Schmidt. I'm a contributor at Bitcoin OpTech and executive director at Brink where we fund Bitcoin open source developers.\n\n2\n00:00:50,420 --> 00:00:54,559\nSpeaker 1: Hi, I'm Merch. I work at Chaincode Labs on explaining Bitcoin to people.\n\n3\n00:00:55,200 --> 00:00:55,418\nSpeaker 0: Gloria.\n\n4\n00:00:56,163 --> 00:00:59,519\nSpeaker 5: Hi, I'm Gloria. I work on Bitcoin Core and I'm sponsored by Brink.\n\n5\n00:01:01,347 --> 00:01:06,479\nSpeaker 3: I'm Dave. I'm the primary author of the OpTech newsletter and commented on a couple of the proposals this week.\n\n6\n00:01:08,022 --> 00:01:24,274\nSpeaker 2: Oh, hey guys. It's good to be here. So my name is Barack. I don't work anywhere. I'm the creator of ARK. I made it public like a few weeks ago. I'm a bit of a free bird, but I'm excited to be here and discuss further. Joost. Hey all.\n\n7\n00:01:24,903 --> 00:01:32,240\nSpeaker 4: So I'm mostly lightning developer. Working on L&D a lot over the past years and recently also gained more interest for Layer One.\n\n8\n00:01:33,043 --> 00:02:38,998\nSpeaker 0: Thank you all for joining us. For those following along, this is newsletter 253. You can look at the tweets shared in the space or bring up the full newsletter on the BitcoinOps.org website. And we'll just go through that in order. The first item of news this week is a proposal for managed join pool protocol. And we have Barack who's the author of this mailing list post and the idea for ARK, which I think was previously named or unnamed TBD XXX. So Barack, thank you for joining us this week. Maybe it would make sense for you to provide a high level overview of the protocol and how it works. And then I think we can get into some Q&A. I know Dave had some feedback on the mailing list and he was kind enough to join us here to walk through some of that feedback and potentially clarify his thoughts and some of the technicals of the proposal. So Barack, you want to?\n\n9\n00:02:39,100 --> 00:18:56,493\nSpeaker 2: Yeah, sure. Yeah, sure. Guys, it's Barack again. I'm the creator of ARK. I think posted a mailing list. The idea 10 days ago, it's an idea, right? It's a new kind of layer 2, soft chain scaling solution layer slash privacy tech idea. And it's in the early protocol iteration phase. I made it public on stage like 10 days ago, same day, posted the mailing list, had some feedback. People are excited, equally skeptical, which is great. So I think so far I did a terrible job communicating the idea with the broader community, I think. But I'm not a great communicator by any means. But what I can tell, I think I'm very confident that it's a great piece of tech and it can scale. It does a better job in terms of privacy and scaling compared to Lightning. There are some trade-offs, obviously we can come to that. But it's something I've been working on over the past six months, more or less. So I mean, perhaps some of you know I did some covenant research and development on liquid for about two years. And about a year ago, I shifted my focus to sort of Bitcoin only, Lightning space. I wanted to explore Lightning and see what I can do with Bitcoin based on my experience in covenant research and development on liquid. On liquid, I shipped like a... It was me and a friend of me, we built an AMM on liquid. Obviously, in Bitcoin, we don't have covenants, so we are restricted. I mean, the script is a primitive language, it's intentionally kept limited. And about a year ago, I sort of wanted to address these problems, lightning pain points. Lightning has many pain points and has a huge entry barrier, the friction. End-user friction and entry barrier to onboarding people. So these two have always been two main concerns. And I think I started working on a Lightning wallet idea like three months ago to address these frictions. And I tweeted about it, it got some good reception. And obviously, I mean, the idea over time evolved into the wallet idea, evolved into a new layer two, like a new layer two, distinct layer two protocol. But it's still Lightning, right? Org is Lightning. I mean, Org started off as a Lightning wallet idea and it's still Lightning. You can pay invoices, you can get paid from invoices. It's like, I like giving this subnet analogy. Org is more like a subnet of Lightning. You can forward HDLCs to broader Lightning and get paid from broader. But internally at the core, it's a different kind of design. I don't know what, it's not state channel, it's not roll up. It's like a third category. I don't know what the name for this category should be. Some people give the coin swap analogy. Some people give, I don't know, there's a few other stuff. But I'm not aware of these like coin swap and all that. Again, I've been exploring Lightning's face for the last one year, more or less. And so, I don't know, Org is like a different kind of thing. Maybe it, but yeah, it has similarities with other protocols. I mean, it has similarities with eCash. It has similarities with Lightning. It has similarities with coin joins. It has similarities with two-way pack sidechains. You can get a bunch of knowledge, yes. Maybe to start, it has similarities with eCash because it's based on VTXOs, virtual transaction outputs. So we have a virtual UTXO set. We lift UTXOs off the chain. And just like how on-chain funds flow, you destroy coins, you create new coins. And these coins are short-lived. We give each coin like a four-week expiry. Unlike on-chain, when you have a UTXO, you can go offline forever. It will remain there, right? Unless you're losing your keys, there is no expiry for UTXOs. Here, they expire at some point, which I don't think it's a bad trade-off at all. Like if something goes wrong, a boat accident or inheritance-related issues, you can at least try to reach out your service provider and claim for a refund. But besides that, we give a coin expiry for some good reason, to make the off-chain footprint minimal when we want to redeem them, when we close these sort of like VTXOs or LUT channels. I like giving this analogy to like Lightning is a two of two, VTXO is also a two of two. But the main difference is in a channel, in a two of two, you sign a bunch of channel state updates, right? Thousands of state updates, and then you can settle back on-chain. On AURIC, it's also a two of two, just like a channel. But think of just signing one state update, just one from the two of two. That state update gives the ownership, it changed the ownership state. So, whether it's belong to, is it belong to me or does it belong to the service provider, one or zero? So, it's kind of like sending all channel funds into your channel partner in one. And then you push your liquidity into server. I mean, we call this server AURIC service provider. It's like an atomic single-hop payment. On Lightning, it's like you can forward payments across multiple hubs. On AURIC, it's a single-hop. It's like atomic single-hop. Lightning is also great for inner-hop settlements. You can open a direct channel with a large hub, or you can exchange to exchange, transfer the inner-hop stuff. AURIC cannot do it. AURIC is like from one party to the other and hub in the middle. So, AURIC is like a channel, but you push all channel funds to the service provider, and service provider push equivalent number of funds minus liquidity fees into the recipient end. You know, just like a single-hop Lightning payment. But now you're pushing entire money or entire funds. The service provider. So, it has similarities with sidechains too. Like, I mean, I'm talking about trust us to way pack sidechains, hypothetical sidechains. It's like you have, you pick in to protocol this piece of different software protocol layer two. You pick in, and pick in stands for lifting your UTXOs. You can pick in this protocol with your real UTXOs, and then you get one-on-one virtual UTXOs. And this is what pick in is for. And when you pick in, you're in the system. And once you're in the system, while you have a VTXO, you're running a piece of like wallet client software. It does coin selection, and it joins a coin join session or coin swap session. So, like you join a session, the service provider is also the blinded coordinator. So, service provider is three things. They are blinded coordinator, coin join coordinators. They are liquidity providers, and they're also Lightning routers. But their main job is to do the coordinated blinding, blinded coin join route. So, you want to make a payment, you're client coin selects, VTXOs that you're spending, you join the coin join session, the next coin join session of your service provider, then you have a new session in every say five seconds, five seconds. Again, it's a bit of a, it's kind of an arbitrary number. It's subject to changes. In fact, it's custom config, and it can adjust the fee market conditions. And you join a coin join session, just like how a coin join works, you know, like input registration, output registration, assigning phase, three phases. So, you coin select, you register for your VTXOs, you know, under these different identities each, and then you get blinded credentials minus liquidity fees. And then you join the output registration, register for this payout VTXOs, but these VTXOs are also under a shared VTXO. And then in the signing phase, you anchor your VTXOs that you're spending into this coin join pool transaction using anchor time contracts. And the coin join transaction ends up on chain, hits on. chain is very minimal in size footprint minimal. It's like 500 B-buys, one or more inputs, three outputs. I think we can lower this down to two outputs also. And then you have a new coin join, new on-chain transaction every five seconds. You know, while this may seem not footprint minimal at all, it is footprint minimal. I consider this footprint minimal because you can onboard a bunch of like millions of like these TDTXOs and millions of recipients for that particular coin join arounds, perhaps even more theoretically speaking. So, upper bound limit, like on-chain upper bound limits. I mean, the upper bound limit like is in theory infinite, but there is practical limits such as bandwidth and computation and all that. So, there is some upper bound limit on this obviously, but it scales a lot better. Like you can make a payment in a coin join. Obviously, non-IMT set is everyone who involves in this payment. Unlike the coin joins, they're mostly used for drug market use. I mean, that drug market use case here. Yeah, it can also be used for that. There's nothing we can prevent that. Like, yeah, it's also payments. The non-IMT set is all, I mean, the non-IMT set is as large as the payment volume, so which is great. And you make a pin join in it. So, you make an off-chain swap out, so to speak. Like you make an off-chain to on-chain swap out. And that swap out is an aggregate transaction by itself. It contains a bunch of other swap outs under the shared ETF. Maybe that's a cool, good one-liner. Obviously, you have the privacy benefits because it's a blinded coin join. The service provider in the middle cannot tell who the sender and the recipient is. Obviously, it's also footprint minimal. Also, it's also convenient to use because we don't have inbound liquidity here. Because you're making a swap out every time, off-chain to off-chain swap out, off-chain to on-chain swap outs. You don't need to, you can receive without a second thought. Maybe we can come to trade-offs later or can talk about trade-offs also. Obviously, there are some trade-offs. The main one, there are two main trade-offs. Main trade-off is, it's not, I mean, ARK payments. So, if you're using ARK to accept payments, which you shouldn't, in my opinion. If you're a vendor, if you're a merchant, Lightning is better suited for you. You better use Lightning. Because if you're a vendor, you know, you demand instant settlement. You don't want Froude, you don't want chargeback. You demand instant settlement. I mean, hypothetically speaking, in a hyper Bitcoin as well. So, and you're always self-hosting your POS terminal, whatever. So, you demand instant settlement and your cash inflows are predictable. You can acquire liquidity, rent liquidity according to your needs. And you can obviously run this piece of specialized software, whether it's BTC space server or, you know, Lightning node in your POS terminal. You can do it. It's okay. You're a merchant, right? But for users, well, it's not. It's chaotic. Like you cannot predict what you're receiving in the first place. Like we're humans, right? We don't know what we're receiving. Like sometimes we receive ZAPs, sometimes donations, sometimes remittances, sometimes regular money transfers, sometimes a payroll from a payroll service. It's unpredictable what my cash inflows are going to be. And it's also subject to denial of service. Like if I am like someone, I just got on board to Lightning. Like in a hyper Bitcoin as well. I'm on board to Bitcoin. I got an orange pill, whatever. And like I have to reach out to someone like a node on the protocol to ask them to open and beg them to open a channel to me like inbound liquidity. Because I can only draw this channel. And but I can be a bot. I can just lie. I mean, well, you can pay some fees upfront, but really like I am asking a channel to me and these channel funds. I mean, it's unpredictable how I'm receiving on this channel. Like talking at a large, large scale. It works today. Yes. But it's large scale. Like I cannot, I can only promise. I can only promise to utilize these channels once my inbound liquidity, but probably not. Probably not. I may not be a hundred percent utilizing, but most likely it's not going to be enough for me to receive because it's unpredictable what I'm receiving. And what happens is when you don't have enough liquidity, you receive a submarine swap. But submarine swaps are inherently unscalable. Anything that touches the chain is unscalable. And I always think in terms of footprint, I always think in terms of like global massive adoption, lightning may work today. Yes. Because we don't have many users. Blockspace can handle that perfectly for today. You can open channels. You can ask to open channels and you can, you know, open channels charges. Great. Lightning is great, but at a large scale, I don't see, I fail to see how lightning can scale at a large scale. But I can see how ARK can do it. because ARK, we don't have channels. It's not state channels. You don't ask someone to open a channel to you. We don't have inbound liquidity. You receive what the sender has and liquidity provider provides that equal number of liquidity to the coin-join transaction. So all lightning service providers provide liquidity to channels on ARK. ARK service providers, they provide liquidity to coin-join transactions. So the different sort of designs. But one thing that lightning and ARK has in common is they are both liquidity protocols. They're both liquid networks. LSPs provide liquidity to channels. They provide liquidity. ARK service providers, they also provide liquidity. And the second trade-off is obviously yes, ARK uses liquidity less efficiently compared to lightning, although liquidity is 100% utilized. On lightning, you cannot promise it. Like you can promise that you might open a channel to someone. Liquidity is your own end, but you cannot promise like the other guy just promised. You're utilizing these funds on ARK. They're entirely utilized. If sender is sending money to recipient, XSATs, liquidity provided provides XSATs equal number of liquidity. So it's entirely utilized. Also lightning is like a multi-hop payment sort of stuff. You roll payments across multiple hubs. On ARK, because it's a single-hop, like single-hop payment, atomic single-hop payment schedule, you only have one hub and liquidity is only deployed on that one hub. So I think like yes, ARK, it may seem that ARK, yes, it is capital slash liquidity requirements are higher significantly, which is true. It's like five to I think three to five and maybe perhaps 10x higher because you're locking up liquidity like one way only. Like on lightning, you have a channel, you can move money in that channel forever. It's like you have to provide liquidity on an ongoing basis and constantly providing liquidity for four weeks because you're locking up every single time your liquidity to be unlocked four weeks later. And of course, this four-week timeframe is also like a bit of an arbitrary number. It is adjustable, like it's manual config or it can adjust to liquidity capital conditions. But just to keep things simple, I just made up these numbers, four weeks and five seconds. So yes, ARK, you're locking up liquidity for a long time for an obvious reason. But the trade-off there is yes, you're locking up liquidity. But because liquidity is entirely utilized, 100% utilized, and because recipient is always one hop away from the center, I think ARK may use liquidity as efficient as lightning. Obviously, you have this convenience of imbal liquidity you receive without having any setup. Like you just download the software, an ARK wallet, and all you have is a Bitcoin address, just like how an on-chain address, an on-chain wallet UX works. And ARK mimics the on-chain UX, obviously. You destroy vTXOs, you create new ones, you have an address from which you can get paid. It's a dedicated address and it's going to be your NPUB. So it's like a pay to NPUB. We're using silent payment style tweaking. So whenever you send a payment, you know the NPUB public key, the selection of public key, the recipient, and you add a tweak, add an ephemeral value to it, and send this ephemeral value to recipient out of band. Oh, I think... Go ahead.\n\n10\n00:18:57,043 --> 00:21:08,359\nSpeaker 1: May I jump in here? So you've given us a pretty broad overview of your proposal, and I would like to try to summarize it a little bit. What I took away from it is that you can have very fast off-chain payments between participants of the same ASP. They have great privacy, even against the ASP, because the payments are mixed at every hop. It onboards users very easily because you can receive without making an on-chain payment first to open a channel or something like that. You can directly become a user of an ASP. And then on the downsides, I have a couple of questions. So one is, so you want to have a responsive design or responsive UX where payments between participants on the ASP are going to be settled or not settled, locked in pretty quickly. And for that, you propose that there's this pool transaction every five seconds. So I see how this will work very well if you have a lot of users, and it will scale well, because then the cost of the pool transaction is, of course, shared by all the participants that send a transaction within those five seconds. But what I don't see is how does an ASP jump from not having any users to having enough users that they can pay for this every five second pool transaction? Or even if you make it a little longer than interval, you need at least one or two payments within the time frame. But if you only get one payment and then try to be responsive, the payments will be fairly expensive at first in order for the ASP to even break even on their cost. So how does your proposal go from zero users to being sustainable cost wise? Is it sort of like you need a lot of investments at first to scale it up and then once it's big, it'll carry itself? Is that the idea?\n\n11\n00:21:09,401 --> 00:23:13,227\nSpeaker 2: So the on-chain fees, yeah. So who covers the on-chain fees? Let's address that. It's the users who covers them. So users pay two fee tiers when they make a payment. They pay, they cover the on-chain fees for themselves plus the liquidity fees, the liquidity provider chart. So liquidity providers are safe. Liquidity providers are not paying the fees for their own pool transactions. They charge fees from their users. And if there is no pool, like if there is no users, there is no pool transaction. Obviously not. If there is no CoinJoin users, there is no CoinJoin transaction. But if there is at least one user who wants to make a payment and joins a CoinJoin session, there has to be this CoinJoin transaction. And because if that CoinJoin transaction is minimal, like 550, 60 V bytes in size, and that user, that single user pays fees for it, which is like paying a regular Bitcoin on-chain transaction, which is not a bad deal. Obviously, if you want to run an ASP, you have to run an uptime server with some locked up liquidity, with some on-chain funds available to you, like available on-chain funds to provide liquidity further. You have to make sure you have enough funds to provide liquidity for the next, say, four weeks on an ongoing basis. But if you end up having no users, well, you're perfectly safe. You're not covering the on-chain fees. Users cover them for themselves. And if there is no usage, you just unlock your liquidity after four weeks. There is no risk for you. And the initial onboarding, yes, like if you have only one user, well, you're not, I mean, FIB users, you're not making much of a profit. Yes, same goes to Lightning, too. But ASPs are perfectly safe to make that clear. On-chain fees are not paid by them. And obviously, the more users we have, the cheaper the fees become on-chain fees-wise, because the fees for that shared ETXO full transaction are shared among other participants. If on-chain fees are like $10 and we have like 100 participants, each pay $1. If we have 1000 participants, each user pays $0.10 for the Bitcoin. No, I understand that.\n\n12\n00:23:15,562 --> 00:23:52,920\nSpeaker 1: What I don't understand is like, where does the first user come from? If you only have a single user within a time frame, the coin join is not going to mix anything, and they have to pay the whole fee. So you would want to have multiple users, so the fees are shared, so the coin join is useful. But what's the incentive for the first person to start using it? So it's both more expensive and less beneficial at first, while there's few users, while it only becomes beneficial and cheap to use if there's many users. So I think it has a bootstrapping problem. You see what I mean?\n\n13\n00:23:53,360 --> 00:24:35,099\nSpeaker 2: So yeah, actually, like I designed ARK as a Lightning mode. You can think of it as a Lightning mode. You're just like a user, like one user who you're downloading a Lightning wallet, ARK wallet, yes, you deposit Bitcoin to it. Well, you pick into it because when you onboard first, there is no VTX in the existence. So you'd pick in on your own, picking in is as simple as funding an on-chain address. And what you can do is just use ARK wallet to pay Lightning invoices to like in a Starbucks, an El Salvador, whatever. Really, just pay Lightning invoices. Obviously, for the first user, you're not using it for the coin join, mixing your coins or doing internal transfers because there is no other user. You're using ARK solely to make Lightning payments.\n\n14\n00:24:35,866 --> 00:25:10,256\nSpeaker 1: Oh, that's cool. Okay. That's what I was missing. Sure. If you start using it as a Lightning wallet, you have already an incentive to make a payment every once in a while, which you can also use to double down as your refreshment payment and get around the four-week timeout. And then you do join the pool transaction. You get the privacy benefits. If there are more users, it will still be somewhat more expensive at first though, to be part of the service, right? Because currently we're not paying $1.50 or so for Lightning. Yeah.\n\n15\n00:25:10,320 --> 00:25:15,339\nSpeaker 2: So if you're the first user, you have to pay like a regular on-chain transaction, just like a web, like an on-chain wallet pays.\n\n16\n00:25:15,761 --> 00:25:41,976\nSpeaker 0: I want to take the opportunity to bring in Dave Harding, who did the write-up for this item this week and also had some interaction with Barack on the mailing list. Dave, obviously we've gone through some of the overview here. I want to give you an opportunity to one, clarify anything for the audience that you think would be valuable to clarify. and two, ask any questions of Barack that maybe you didn't get to on the mailing list so far.\n\n17\n00:25:43,340 --> 00:26:39,480\nSpeaker 3: Sure. Absolutely. So Barack, first of all, thank you for making the proposal. I always like reading about new stuff like this. It's a very interesting idea. So in a lot of your discussion about this, you compare this to Lightning. And so I've kind of looked at it through that lens. And you were going into this earlier. I think we may have got a little sidetracked, but the downsides of this proposal, and you were saying that perhaps merchants shouldn't use this for receiving their own payments, is that when a payment is sent using ARC, it needs to be confirmed. The payment that the service provider makes, the ARC service provider makes, it goes on chain, it starts out unconfirmed like any other transaction, and it gets confirmed. So it's security for a third party, someone who doesn't trust the service provider, someone who doesn't trust the sender. The security of that payment depends on on-chain confirmation. Is that correct?\n\n18\n00:26:40,661 --> 00:30:50,980\nSpeaker 2: Oh yeah. So by the way, that's correct. The senders, by the way, cannot double spend it unlike on-chain payments. On-chain payments can be double spent by the owner of the UTXOs, inputs. But here it can only be double spent by the service provider because the coin joint that hits on-chain has one or two inputs and is the single SIG owner of these inputs. Apart from that, yes, if you're a merchant, yes. I mean, you and IBI, ideally you're not trusting anyone. That includes the service provider you're choosing or using. I mean, the sender actually is using. Yes, you should not accept payments ideally in the short term to accept ARC payments unless we have like this penalty mechanism I mentioned on the Bitcoin mailing list. But we can come to that in a second. But Lightning is again, yes, better suited for payments because you have instant finality on Lightning. Here you don't. Finality, in order to consider a payment final, quote unquote, final, you need to wait for on-chain confirmations. For vendors, yes, this is a risk because vendors do not want chargeback. But for end users, it might make sense. I mean, obviously, if you're an end user, you should wait confirmations too. But what you can do, I mean, ideally you receive payments from friends or family anyway. But also you have, when you receive funds, like from an end user standpoint, like you receive funds from someone you don't know, it doesn't really matter. Like the funds are available to you right from there. Like how you can change unconfirmed transactions. Like in a mempool, you can hand over VTXOs, how you can hand over zero-conf UTXOs to friends and others. And you can in fact pay Lightning invoices with them. So I receive a VTXO in a coin join. It's not confirmed. It sits in mempool. It can in theory be double spent by the service provider, yes. And I don't consider it final. But the funds are available to me and I can pay Lightning invoices with them through the same service provider. Because service providers, again, they're also LSPs. And the service provider was the only guy in the end, only a party who can double spend my transaction, my incoming transaction. But the same guys, same end party is also the Lightning router. And I wrote payments through the same party with the corporation. So for vendors, yeah, I think you should not accept payments on Oric. Unless we have a penalty mechanism, I should follow up with our convo, a mailing list on this. So if we have a hypothetical opcode called Opex-JAR, the Bitwise Logic opcode, previously disabled or OP-CAT concatenation opcode, we can constrain nonce in a signature. This is hypothetical future extension thing. But I've made my peace with that. I mean, I think Oric is great as it is. But if we are to bring a penalty mechanism, we need these opcodes to constrain nonce. And if you reuse their nonce, the ASB reuses their nonce. in a pool transaction, anyone can fraudulently public key. But I can, as a user, redeem, like claim my previously spent VTXOs. Just like how inbound liquidity works, you can penalty the channel liquidity. Here you can penalize your channel partner. In this case, channel partner is your liquidity provider, service provider. I can forge signature from the two of two because VTXO is a two of two with a time lock. Time back to myself. And from that VTXO, the shared VTXO also has a time lock. Back to the service provider. for a big time out, for a big time lock. And I can, because the form, and I can't afford signatures from the two of two to claim my funds. The service provider cannot collate with the miner. If service provider is a miner themselves, like your concern in the mailing list doesn't work because the time lock is not over yet. The four week time lock is not over yet. And within this time frame, four week time frame as a user, I'm the only guy, I'm the only party who can forge signature from the two of two because the two of two collaborate the path and then VTXO gives them high precedence.\n\n19\n00:30:51,700 --> 00:30:51,791\nSpeaker 0: Dave?\n\n20\n00:30:53,361 --> 00:30:59,139\nSpeaker 3: I didn't quite catch all that. I think we can probably save that particular discussion for the mailing list.\n\n21\n00:30:59,560 --> 00:31:01,318\nSpeaker 2: Let's follow up on the mailing list.\n\n22\n00:31:01,962 --> 00:33:29,519\nSpeaker 3: Yeah. So I guess I was just trying to think about this in comparison to Lightning. So your concern here is to understand. here is a big part of this is liquidity for everyday users, right? That's your concern here is right. And so I'm just thinking through Lightning and I think we kind of already have the feature set there that you're looking for here. So let's say we have two average everyday users. We have Alice and Bob. Alice wants to send a payment to Bob, but Bob doesn't currently, and they both use Lightning, but Bob doesn't currently have enough liquidity to receive Alice's payment. And so what Alice can do is she can open a new channel to Bob and she can do a push payment is what I think they call it in Lightning, where the channel is opened in a state where Bob receives all the funds. So I just want to walk through this really quick, how this works in Lightning. The listeners is that in Lightning, a funding transaction is just a two of two multisig. It's kind of like exactly what our arc is a two of two multisig. And as long as Alice knows Bob's public key, she can create that funding transaction without any interaction from Bob. She can just create an output for a good transaction as a regular Bitcoin transaction. Bob doesn't need to be online at the time. That transaction can get confirmed without Bob's input. And then the initial state of the channel, Alice can create that again offline without Bob's participation and then send it to him through an async communication method like email, just an out of bank communication. Again, like arcs, you can send that initial state, which is just a signature. It's just some data and it's Alice's signature saying, Bob, here's one BTC in this initial channel state. And so for Bob to trust that he has to wait for confirmations. And so ignoring the scaling aspect, which I can come back to later, but ignoring the scaling aspect, I think this looks very similar in user experience to arc. Bob can trustlessly receive money on Lightning Network by waiting for a certain number of confirmations. It doesn't require his interaction and it can be done using out of band communication. What do you think, Burek?\n\n23\n00:33:30,261 --> 00:33:42,259\nSpeaker 2: Okay, so you're saying Alice is a user, Bob is a user and Bob, I think, doesn't have any open-mail liquidity, so Alice open the channel to Bob. But is that correct? Or Alice is a service provider or something, or Alice is just an end user?\n\n24\n00:33:43,222 --> 00:33:46,959\nSpeaker 3: Alice is just an end user. She's just an every... Alice and Bob are just everyday users.\n\n25\n00:33:47,420 --> 00:34:47,320\nSpeaker 2: Great. But do you guys realize what kind of UX assumption is this? Alice is an end user and Alice is going to open a channel, like literal channel to Bob. Channel liquidity management is already a big problem and we get... Asking Alice to this wallet software is opening channel to Bob and assuming Alice has enough on-chain funds. I mean, opening channels and closing channels, they don't only scale, not in terms of on-chain footprint and not only in terms of convenience, but literally, because of the inbound liquidity problem. what you're describing is, yeah, one way to mitigate it, yes, it might work in theory, but in practice, a user opening a channel to other user, it just doesn't work. And that's what literally arc tries to solve. Like offloading complexity from end users, end users should not deal with anything, no complexity, no channel management, no nothing, no management, no liquidity, no concern. And let's offload this entire complexity to the service provider in the middle who can take care of everything for you, yet as a user, I retain my sub custody. That's literally what arc tries to solve, like offloading the complexity from end users.\n\n26\n00:34:49,201 --> 00:35:53,199\nSpeaker 3: I guess I just don't understand where the complexity here is because Alice, first of all, she needs to have funds to pay Bob. That's going to be true with any trustless protocol. She already has to have the money. And that money can be for Alice, it can be on-chain and her on-chain wallet, or it can be off-chain with splicing, which is where we're getting pretty close to splicing in at least two or three implementations of LN. So Alice needs to have the money to pay Bob. And if Bob doesn't have a channel, whether he's using arc or he's not using arc, he's just using regular LN, he's going to have to wait six confirmations or however many confirmations he wants to receive that money. So I think the UX here is exactly the same, whether we're looking at arc or we're looking at lightning. It's not a liquidity management here because Bob doesn't need to have liquidity to receive an on-chain transaction. And a channel funding transaction is just an on-chain transaction with some extra data sent out of band for that initial state.\n\n27\n00:35:54,240 --> 00:38:57,619\nSpeaker 2: So what you describe involves a loss of friction. The main friction being assuming Alice has enough on-chain funds to open a channel to Bob. And that's a big, big, big assumption, UX assumption. I think in an ideal world, everyone should have one unified balance, an off-chain balance, not on-chain because it's not convenient. Also, it doesn't scale. Opening channels do not scale. Also, there's another thing from Alice. to opening a channel to Bob, obviously, Bob needs to wait for confirmations. Yes. And that also goes to ORIC, but on the scheme you're describing, for Bob to receive money, yes, it has to wait confirmations and it has to wait confirmations to forward the payment that payments further. On ORIC, the user, end user doesn't have to wait confirmations to wait the payment further, forwarding as in like paying another invoice, paying or making internal transfers because ORIC uses ATLCs instead of HDLCs. A double-spend attempt breaks the atomicity from there. A double-spend attempt, if there is a double-spend attempt, the sender, in this case, always gets refund. On ORIC, ORIC receives the refund too. But on ORIC, if there is a double-spend attempt, so on ORIC, like, I there is no pre-image, right? We don't have HDLCs, we have ATLCs. And as an entity, Bob, in this case, like in the same example, ORIC, always opens a channel to Bob. In this case, Alice is the service provider, not an entity. That's one thing. There is no friction. And again, a central hub takes care of it for you, not an end user. End user doesn't have to have any hustle, no nothing, no friction. And a service provider takes care of it for you, that channel opening, so to speak. And that's kind of how ORIC works too, yes. Like you have a coin join, coin joins, and you have a new sort of channel in a coin join, yes. So end user is not taking care of it. And the channel, like, I'm receiving funds from Bob. I'm receiving some funds from Alice, the service provider. And I don't have to wait for confirmations. I can literally pay a Lightning invoice with them, just like how I described. This doesn't work on Lightning, because on Lightning, someone opens a channel to me, the channel is zero conf. If I'm not waiting confirmations, obviously, if I want to make payments, like another payment with those funds, and I don't want to wait for confirmations, obviously, because if I want to make a payment, that channel liquidity has to be available on my end first for me to make another payment. And in order for this to work, I mean, I don't want to wait. If I'm not waiting, I have to reveal my pre-match, reveal my pre-match for the payment. But when I reveal my pre-match, the sender, like, Alice in this case, can double-spend my channel, the channel she opened to me, or it could be a service provider, doesn't matter, yet takes my money, yet the sender's money, because I revealed my pre-match. So it's like a double-spend. On Arc, it's not the case. There is not only, like, one entity, like Central Hub, taking care of it for you, like the channel liquidity management. Also, it's convenient to use, because you don't have to wait confirmations to forward a payment further.\n\n28\n00:39:00,023 --> 00:39:46,019\nSpeaker 1: I'm not sure I follow this argument, because if someone opens a channel to you, and they fund the channel, and send you some of the initial balance, and you make a payment through that channel opener, the only money and the only counterparty that can lose money is the other side, because you're gonna pay through them out. So they're basically giving you credit, and you are using that credit immediately. But I think we are already 45 minutes in. I think we would want to wrap the discussion a little bit. So if you both have some concluding thoughts on the debate or the conversation, maybe you could try to move towards a final thought.\n\n29\n00:39:47,532 --> 00:39:47,680\nSpeaker 0: Sure.\n\n30\n00:39:48,160 --> 00:40:11,679\nSpeaker 3: I think, again, I'm just gonna say this is a really interesting proposal. I think it's gonna be fun for the next few weeks for us to explore the edges of it, and see how it compares to the existing solutions. And I'll try to post on one of the mailing lists my thought for how Arc compares to channel funding, so that maybe Burke and I can go into more detail later. So thank you, Burke, for this really interesting idea.\n\n31\n00:40:11,981 --> 00:40:18,826\nSpeaker 2: All right, cool, guys. Thank you. I'll follow up on our previous conversation from a week ago, and we'll move from there. Sure.\n\n32\n00:40:20,269 --> 00:40:20,779\nSpeaker 1: Thank you both.\n\n33\n00:40:21,100 --> 00:41:29,940\nSpeaker 0: Yeah, thanks for joining us, Burke. And folks who are interested can go to arctil.me for more information, and also check out the mailing list post. And I know Burke is working on a GitHub repository for some of the documentation that people are interested in. So keep an eye out for all of that, and we will cover in the OpTech newsletter accordingly. Next item from the newsletter is transaction relay over NoSTR. And Joost, you posted to the Bitcoin Dev mailing list some prototype that you had been working on, based on an idea from Ben McCarman about using the NoSTR protocol for relaying transactions. And I know folks most commonly may be familiar with NoSTR as the basis of decentralized social media platforms, but can also pass messages that are not necessarily related to social media posts. And in the context of this discussion, it sounds like some of the messages that could be passed around are Bitcoin transactions or a group of transactions like packages. So Joost, do you want to explain the idea a little bit more, and the prototype that you have going?\n\n34\n00:41:30,684 --> 00:43:12,837\nSpeaker 4: Yeah, yeah, definitely. So for background, the way you could see this is that traditionally, Bitcoin transactions are relayed across the Bitcoin peer-to-peer network to miners. And apparently, but I'm not one of those people, there are people that are friends with miners, and they're able to send them transactions directly by email for inclusion in a block. And I thought, wouldn't it be great to have also like alternative means to reach those miners that are accessible to anyone? And then the Carmen, he came up with this NoSTR standard to relay Bitcoin transactions. And also to me, that looked like quite like a suitable alternative transport mechanism, but it doesn't need to be NoSTR necessarily. Like yesterday, I started experimenting with transaction relay over Twitter. Like why not? Like somebody also commented there that maybe miners should be sort of scavengers, just scarring the internet, trying to find anything that pays fees, that allows them to build a better block and not restrict themselves to P2P and possibly email, but just look wherever transactions show up. So, but this NoSTR idea, I think it's particularly interesting because in NoSTR, there's also ways in the protocol itself to do anti-DOS. So there are NoSTR relays, for example, that require a fee to be paid if you want to post there. And if you misbehave, I assume your key will be banned, something like that. So yeah, if you can reuse that functionality that already exists to make this relatively safe to do, seems like a good option. But the main idea here is just to explore alternative relay mechanisms for Bitcoin transactions to not only rely on the P2P network.\n\n35\n00:43:13,561 --> 00:43:13,840\nSpeaker 0: Gloria?\n\n36\n00:43:15,290 --> 00:43:17,319\nSpeaker 2: Hi. Yeah, I found this really interesting.\n\n37\n00:43:17,620 --> 00:43:42,059\nSpeaker 5: So thanks for working on this. And I just kind of had some like clarification questions. So are you thinking like NoSTR would be another decentralized network that would relay transactions? Or are you thinking just kind of more methods for miners to like receive or have people submit transactions to them directly?\n\n38\n00:43:43,200 --> 00:44:05,239\nSpeaker 4: Yeah, I think, well, both actually. So like an alternative decentralized network for transaction relay. But the main idea, as I mentioned, just to provide alternatives, increased resiliency, let's say there is something with the P2P network that makes it so that transactions cannot be propagated for whatever reason, then there are like fallback mechanisms in place that work in a completely different way, possibly.\n\n39\n00:44:05,860 --> 00:44:16,499\nSpeaker 5: What kinds of kind of I'm imagining kind of censorship vectors, or maybe just inefficiencies? Like, are there examples in particular that you're thinking of?\n\n40\n00:44:17,763 --> 00:45:06,060\nSpeaker 4: I'm now just going to make this up right now. Just the main idea here was to just. resiliency can't be bad, it seems to me. But let's say it's possible to sort of like block P2P traffic for Bitcoin nodes, or that is like a firewall is instantiated somewhere. If the whole system, like the connection between users and miners is also possible across different transport mechanisms, like that wouldn't be like an instant problem. You can just fall back to any of the others. Or even better, like anytime you want to broadcast a transaction, you just do it through three different mediums. So you do broadcast on P2P, you do it on NoSto, you do it on Twitter. And smart miners will just look for everything because it increases the chances of picking up the best transactions, even if one of those mediums is blocked.\n\n41\n00:45:06,840 --> 00:45:12,390\nSpeaker 5: And what did you mean by DOS concerns? Yeah.\n\n42\n00:45:13,270 --> 00:45:13,459\nSpeaker 0: Yeah.\n\n43\n00:45:14,390 --> 00:46:16,099\nSpeaker 4: Yeah, I know what you mean. So suppose a miner would just open up an endpoint for anyone to submit transactions to their mempool. Like maybe they are worried that they get like so many transactions flowing in there that they need to start managing that, or basically they are DOSed on their endpoints. And with NoSto, like this is what I mentioned, how this might play out in the future. You have these relays and they are already trying to specialize in DOS protection. Like they also do this for social media, I believe. At some point there were a lot of free NoSto relays and they were spammed heavily and they started experimenting with these lightning fees. So if you, as a miner, only connect to a NoSto relays that has some kind of protection in place, and I would also say that if you connect to Twitter, you're basically relying on Twitter as the anti-DOS mechanism. Like if you want to use Twitter, you need to, I don't know what you need to do, give your phone number or something like that. So the idea is the same that there is like another service that makes sure that the traffic, the flow is filtered and then you just subscribe to that. So you're doing as miner, you don't need to worry so much about that.\n\n44\n00:46:16,680 --> 00:46:24,820\nSpeaker 5: I see. So you're thinking of DOS mostly as like computational resources that could be exhausted through validating transactions?\n\n45\n00:46:25,420 --> 00:46:26,620\nSpeaker 4: Yeah, yeah. That's all I was thinking.\n\n46\n00:46:27,880 --> 00:46:31,720\nSpeaker 5: And network bandwidth, I suppose it would be kind of included in that.\n\n47\n00:46:31,920 --> 00:46:32,000\nSpeaker 4: Yes.\n\n48\n00:46:32,562 --> 00:46:32,712\nSpeaker 2: Okay.\n\n49\n00:46:33,400 --> 00:46:34,257\nSpeaker 0: Murch, do you have your hand up?\n\n50\n00:46:34,680 --> 00:49:10,297\nSpeaker 1: Yeah. I have two thoughts on this proposal that I thought I'd put out there. So one is, I think that you mentioned in the context of your proposal that one of the concerns is of course, if you have commitment transactions that cannot be relayed on the network because their own fee rate is under the eviction fee rate. And thus most mempools just don't even accept that transaction into their mempool. And it's impossible to see if PETA transaction, because when the parent transaction doesn't make it into the mempool currently, we will not accept the child transaction. So clearly you're concerned about us being unable to relay packages and that being a detriment to lightning nodes right now, especially with the fee rates and block space demand being all over the place. So I think I appreciate that concern and Mo Blur has been working on this for a couple years now to fix in full on the main net, but clearly that's also still taking time and is not quite where we want it to be. So in a way, I perceive this as a rallying call to put more resources toward fixing package relay. The concern that I have with this proposal is I think for the Bitcoin network to remain as decentralized as possible and especially to maintain the censorship resistance properties that we like in the Bitcoin network, we need to make sure that all the juicy transactions and all the fees that are available in the system are readily available to all participants. So if we want people to be able to jump in and become miners because the existing miners are not confirming the transactions we want and our response needs to be that we need to make sure that everybody gets all the transactions and new mining pool entrance have access to all the fees that everybody else. So in the past few weeks, we've seen proposals for basically private mempools and new out of band mechanisms that would make it harder for miners to learn about all transactions and that might unfairly favor big mining entities because it would be easier to just submit transactions to the biggest three or five. So I'm a little concerned on that aspect. What do you think about that?\n\n51\n00:49:11,500 --> 00:50:25,359\nSpeaker 4: Yeah, I understand what you mean. But I think for lightning, there's already problems today. Like if these high fees people actually run into that in the issue that they couldn't get that commitment transaction confirmed. So basically there's funds at risk. And if you don't have to choose between just letting that problem be versus having like a temporary alternative transport mechanism to get them to miners, even though it might indeed, as you say not provide the global access that the P2P network might provide. Yeah, I think it's still better than having nothing. And also in the case where packages would relay over P2P where all that work is completed. I think there's still value in having redundancy, like having alternative mechanisms, even if they are not as easily accessible as the P2P network, just in case as an insurance. And finally, I don't think this is something that you can stop because if miners are incentivized to pull transactions from as many sources as possible, but what can you do about that? You cannot talk about it, but it might happen in the event anyway. And in fact, it's already happening, isn't it? Miners reading emails containing transactions and putting them into blocks. So yeah, there's also so much you can do in preventing that.\n\n52\n00:50:25,620 --> 00:50:26,633\nSpeaker 0: Dave, I see you have your hand up.\n\n53\n00:50:27,933 --> 00:51:55,799\nSpeaker 3: Yeah. So when I read Yoast Host, what I really thought of as how it would work is that we would use the Noster Relay for exceptional cases, and we would do our best to optimize traditional P2P Relay for average cases. And if we saw a use case growing on Noster, that would be a good sign that we should be optimizing P2P Relay for that case too. We should, it's just kind of a, it would be a feedback loop kind of thing. So that the Noster Relay, although they might end up using it, people might end up using it for all their transactions, we would really want that network, that sort of side network for exceptional cases, people who wanted to do weird stuff or a situation where like, we don't have package relay yet in a deployed node and people need it now. So you're turning to Noster. And in my replies as opposed to the mailing list, I tried to think of ways that we could just kind of solidify that, how to make that more realistic by thinking about how we could just have people, instead of sending individual transactions, just send whole candidate blocks to miners and have them figure out, hey, this is a more profitable block I'm mining right now, maybe this is worth it. That was just my thinking was that we really want to keep P2P Relay. We want to keep that working really well, but it's good to have an alternative mechanism for cases where P2P isn't working for people right now.\n\n54\n00:51:56,460 --> 00:53:06,999\nSpeaker 5: Yeah. I just, could I add onto that if that's okay. There's, I think there's a lot in the peer to peer transaction relay that maybe people are not aware of. that kind of builds towards these design goals we have of censorship resistance and high accessibility of being able to both join the network and broadcast your transaction or be one of the people who takes those transactions and produces blocks. And there's a lot of privacy stuff that's built into our transaction relay. In addition to the kind of DOS and network bandwidth and cash usage kind of concerns that are maybe a bit easier to kind of plug it into something like, like Noster Relay. Yeah. And I agree fully with Harding that if there's a lot of adoption of alternative mechanisms and of submitting out of band or privately, that we should take as a sign to improve the peer to peer transaction network, transaction relay network so that we eliminate these kinds of inefficiencies. The, yeah, ideally package really just works, but we're just not there yet.\n\n55\n00:53:09,400 --> 00:53:18,379\nSpeaker 0: Jost, did you have any final thoughts or things that you'd like folks who are listening now to experiment with or provide feedback on?\n\n56\n00:53:19,271 --> 00:54:30,998\nSpeaker 4: Yeah. So the other thing I've brought up, we've been talking about package relay and maybe getting package relay sooner by using a different mechanism. That's where it's easier to implement at least temporarily. But I think the other thing is about these non-standard transactions. Like we had some discussion in the PRs about that as well. And of course there's like aspects of non-standardness that are good, just protecting this historical reasons, et cetera, et cetera. But there's also non-standard transaction that actually end up in a block. For example, these huge JPEGs or the other one is like usage of the, of the annex and something like the annex. it's perfectly valid to use it in blocks, but it feels like that. it's sort of gate kept by the policy that nodes use. And you could also say it's sort of subjective, whether this is a good thing or not. And yeah, I can also imagine that miners just looking to maximize fees. They would be happy to accept transactions that use the annex, for example, but it's not possible because the P2P network doesn't relay it. So I can also see like some interesting dynamics coming up if you use alternative relay mechanisms that do not have these limitations as much.\n\n57\n00:54:31,721 --> 00:54:40,039\nSpeaker 1: Could you provide an example of how the annex is being used right now? Because that's news to me and probably a bit of an issue with future updates.\n\n58\n00:54:40,940 --> 00:55:27,077\nSpeaker 4: Yeah, it's not, it's not, I think it's not used. Actually I scanned the chain and there's zero like annex transactions that have been done if I didn't make any mistake there, but there are a lot of ways it can be used. So for example, these inscriptions, I think with the annex, you don't need to do the commit review scheme anymore, but you can just put your like additional arbitrary data in a single transaction. You don't need two transactions anymore. So it makes things more efficient on the chain. And I think in various places, there's a whole list of things that you could do with the annex. And indeed, as you say, you could say like, nah, we cannot do that yet because we need to think about what we want to do with it first. But on the other end, the consensus doesn't limit usage of the annex. So maybe if you want to do more thinking, maybe it shouldn't have been introduced in the first place.\n\n59\n00:55:28,822 --> 00:56:36,819\nSpeaker 3: I think one of the reasons we don't enable features like that in P2P relay that might be used in future soft forks is because a miner who is accepting those transactions might create invalid blocks after a soft fork. So if we start using the annex, we start requiring it to follow a particular format and somebody tries to jump a JPEG in there, it might violate that format. And if the miner hasn't upgraded their software during a soft fork, they're going to create invalid blocks and they're going to lose a lot of money. So it's bad for miners to generally ignore that sort of standard policy that's reserved for soft forks. So that's the reason it's not done in Bitcoin Core. Now, if somebody is dumping a lot of money there and the miner wants to enable it on an ad hoc basis, yeah, maybe that makes sense. But as a policy, we want to have a policy that removes as many foot guns as possible and not allowing or not encouraging miners to create blocks that have transactions that might violate future soft forks is one of the ways we remove those foot guns.\n\n60\n00:56:37,622 --> 00:56:44,520\nSpeaker 4: But for future soft forks, wouldn't this limitation then not apply just from a certain block art onwards? Or isn't it? It would.\n\n61\n00:56:45,921 --> 00:56:54,138\nSpeaker 3: But the miner hasn't upgraded their software. They're going to continue to accept and continue to build blocks that include transactions that now violate the new rule.\n\n62\n00:56:55,480 --> 00:57:00,820\nSpeaker 4: Well, it's not that soft works happens like every other month or so that you just don't see it coming, right?\n\n63\n00:57:01,160 --> 00:57:18,379\nSpeaker 5: Yeah. But the idea here is we want a soft work to be smooth and that not everybody has to update on the same day. Or like you could have just like 5% of people like taking a few extra weeks or a few months or something and they would still be fine.\n\n64\n00:57:20,340 --> 00:57:34,899\nSpeaker 4: I don't really understand then why blocks with an annex weren't made invalid initially. Like why was that annex introduced if you cannot do anything with it? But maybe I don't have enough layer one knowledge to really understand this.\n\n65\n00:57:35,020 --> 00:57:58,279\nSpeaker 5: Yeah. So there's a lot of kind of protocol development where it's like, oh, we're going to leave 32 bits to define 32 versions and we're just defining version one. Because we want to leave room to define 31 more versions. And for now, these versions don't have any meaning, but we're giving ourselves room to change things in the future.\n\n66\n00:57:59,621 --> 00:58:45,998\nSpeaker 1: The issue with making it invalid right now would mean that anybody that is not upgraded at the time of the soft fork activating would get forked off the chain when we start using it, right? So anyone that hasn't upgraded in a while would just become cut off from updates from the network. And that's why we generally like to go from everything is allowed to a restriction in our soft forks where anybody that has old software still can follow along because the majority of the hash rate is just enforcing more rules now. While if we first disallow something and then allow it going forth, we will just the people that continue to enforce the disallowance will just be forked off.\n\n67\n00:58:47,040 --> 00:59:01,637\nSpeaker 4: Yeah. Okay. I get that. I get that. But still you could say that the idea that that annex should first be further defined before it can be used. It is subjective in a way, right? Not every Bitcoin user necessarily needs to agree with that and not every miner needs to agree with that.\n\n68\n00:59:02,060 --> 00:59:17,778\nSpeaker 1: That is generally correct, yes. If a bunch of users and miners started using the annex, we would probably have to start making our rules keep that in mind. So it would restrict the design space for future updates if it gets much used now.\n\n69\n00:59:19,260 --> 00:59:30,499\nSpeaker 4: Yeah. So it does feel a little bit like a ticking timer because maybe it's just a matter of time for this to happen. And hopefully before that happens, the desired structure for the annex is in place already.\n\n70\n00:59:31,100 --> 00:59:57,959\nSpeaker 1: I would very much hope that anybody that starts using hooks for future design upgrades would start talking about that on the mailing list and with other protocol developers before making pushes on the network with using these deliberately left for future update things. So I think putting it out there as, oh, this is free to use is sort of a detriment for all of us in the future.\n\n71\n00:59:58,320 --> 01:00:08,738\nSpeaker 0: Joost, thank you. Thank you for joining us. You're welcome to stay on. You may have some opinions on some of the other items we discussed in that newsletter. Thank you for joining us.\n\n72\n01:00:09,120 --> 01:00:09,308\nSpeaker 2: All right.\n\n73\n01:00:09,842 --> 01:00:44,600\nSpeaker 0: No problem. Next segment in the newsletter is related to our series on transaction relay and mempool. This is part three. We spoke about in part one about why we have a mempool at all. And in part two, we talked about how transaction fees are an incentive mechanism for miners to include your transaction in a block that is limited in terms of the amount of block space. And now in part three here, we're outlining strategies to get the most for those transaction fees. Murch, I know you're one of the authors of this week's series. How do you think about bidding for block space?\n\n74\n01:00:45,380 --> 01:02:21,379\nSpeaker 1: Yeah. So as we know, the demand for block space has been a little different in the past few months. And I think that every time we see these peaks of block space demand and the fee rates shoot through the roof accordingly, it drives people to adopt efficiency improvements that have been outlined and available for a while. So for example, in the 2017 run-up, we saw that a lot of people started using wrapped SegWit very quickly and then later also started transitioning to native SegWit because the fees were so high and they just started moving to using a new address standard. So at least the future UTXOs that they were receiving would be cheaper to spend. So in our column this week, we outline a little bit the mechanics of building transactions, which parts of the transactions we have the most leverage to influence when we build our own transactions. We also point out how using more modern output types will save you money, especially when modern output types take less block space so you'll pay less fees for them. And we especially also talk about reprioritizing of transactions. So there are two main mechanisms to do so with CPFP and RBF, and we go a little bit into the trade-offs. So that's just roughly the overview. I think I can dive into more details if you think that's useful.\n\n75\n01:02:21,980 --> 01:02:31,918\nSpeaker 0: Yeah, I think it would be useful. And also I want to make sure that Gloria can chime in as well. I know she was a co-author of this segment. Gloria, any thoughts so far?\n\n76\n01:02:33,120 --> 01:02:38,989\nSpeaker 5: Well, Murch is the coin selection guy, so I think he's the perfect person to talk about this.\n\n77\n01:02:40,042 --> 01:11:52,760\nSpeaker 1: All right, let me go a little more into detail. So when we build transactions, the header bytes of the transaction are basically always required. And they just change a little bit whether you're building a non-SEQA transaction or a SEQA transaction, and then the input counters and output counters will get slightly bigger. if you exceed the magical border of 252 inputs or outputs, then you need a few more bytes on the input counter or output counter. But other than that, you can think of the transaction header bytes on a transaction to be a fixed overhead that everyone that wants to build a transaction has to pay. Regarding the outputs, generally, they're our payload. We want to make a payment or multiple payments, so we know already which script pub keys we need to pay, and we know already what amounts we want to assign to them. We can pick, of course, what output type we use for our own change output, and we can try to avoid a change output altogether if we pick our inputs in a smart way. But generally, the outputs are also very inflexible because those are the things that we aim to create, and thus they're predetermined by the payments we want to make. So finally, the inputs are actually where we have a lot of room for flexibility. With the inputs, of course, we want to be thrifty, especially when the fee rates are high, we want to minimize the weight of the input set. We can do that, for example, by using modern output types. If we use a pay-to-taproot input, that will cost less than half in block weight than, for example, a non-segue pay-to-public key hash input. And if we then are sensitive to the fee rate and how we approach coin selection, we would, for example, use as few as possible inputs at high fee rates and as lightest inputs as we can, while at low fee rates, we might want to be looking ahead and say, if we have a super fragmented wallet, already use a few more inputs and the heavier inputs like the old formatted output types, we would want to prefer using those at low fee rates because we can then spend the more costly and higher weight inputs and consolidate them into bigger chunks of modern output types to save funds in the future when the fee rates get high. So, you could think that you would want to optimize on every single transaction to build the smallest input set, but even back in 2016, when I wrote my master thesis on coin selection, we saw that service providers and wallets that used this strategy would just very brutally fragment their UTXO pool and set themselves up for situations where later when they wanted to create transactions, especially at high fee rates, they would suddenly have no option other than picking a huge input set with dozens of inputs and pay a huge fee. Especially when you have ground down all of your inputs to small pieces and you then want to make a big payment, you just have to include a bunch of smaller pieces where most of it just goes towards the fees of paying for that input and there's little benefit to actually funding the transaction, but you have no other funds and that's what you need to do. So, you want to sort of find a middle path where you are very thrifty when the fee rates are high, but then when the fee rates are low, you're looking ahead and already cleaning house a little bit. The other thing is, of course, I think that especially with the huge adoption of pay-to-tap route now, I hope that more wallets are going to be able to send to pay-to-tap route outputs and also use them for inputs. I think for multisig, there is the biggest savings, obviously, because even with modern output types, you still have the actual two or three multisig, for example, written out in an output and input, and it takes more block space to do so. But if you know that two keys are more often used to make this spend, you can immediately make that your music key path spend and then have the same footprint as a single SIG. So, I'm pretty happy to hear that, for example, services like BitGo already have on-chain support for music and can, with their two or three multisig setup, make payments that look like single SIG on-chain. And they'll essentially, even if their user switches over from pay-to-witness script hash, will save something like 40%, 43%, I think, on each input at the same cost for the outputs. So, I would just say you have to have both a short-term perspective. You want to switch over to more modern output types if you want to save money. And then finally, the game of getting confirmed in the first place means that your transaction has to bubble up to the top of the mempool. Miners generally include everything they see in the mempool from the top one block into their block templates, and when they succeed, you get your confirmation. So, at some point, your transaction has to be among the first block of transactions waiting. But you basically have two strategies getting there. One is you just overpay in the first shot, and then even if there's a slow block, you're pretty sure that you get confirmed quickly. The other way is to start with a conservative bid and then to bump up the priority of your transaction if it takes longer than you want. And for those, we have two mechanisms. One is the sender or the receiver. Anybody that gets paid by a transaction can do a child pays for parent transaction. And this is nice because the TXID of the original transaction doesn't get changed. It's open to the receiver, and it's fairly simple to implement. A lot of situations where your original transaction didn't get through because there was too much block space demand and other people are outbidding you, and now you have to add more transaction data to this package in order to get the transaction through at a high fee rate. So, you add a second transaction that you also have to pay for at a high fee rate, and you're basically already in a high fee rate scenario. So, more efficiently generally is if you can to use RBF to completely replace the original transaction and make a conflict. So, your replacement transaction has to use at least one of the same inputs. Generally, you would include the same payments, maybe even batch two or three transactions together to combine the payments into a single transaction. And then only pay for the payload once, only have a small set of inputs to create all of those transaction outputs in one transaction, and you outbid your own original transactions with a higher fee rate and a higher absolute fee in order to replace them. So, I wish that more wallets would generally build their transactions signaling replaceability and have options to bump transactions directly. So, users can make conservative estimates first and then bump up as needed. And I think finally what we also mentioned in the article was, of course, especially if you have a high volume wallet, you can build your transactions in that manner in the transaction because then you only pay this transaction overhead for the header transactions once and you might be able to get away with a single input for many payments and a single change output for many payments because every time that you create a change output, of course, you incur a future cost as well where you have to spend that UTXO later too. So, if you split up many payments into separate transactions, every time you pay for the transaction header and almost every time you'll pay for the change output and have to spend the change output later, but if you make a batch payment, you only get that. You share that overhead cost across all payments. Yeah, sorry. I've been talking a lot. I think I've covered most of what we wrote in our article. Did I miss anything? Any questions, comments?\n\n78\n01:11:53,960 --> 01:12:22,486\nSpeaker 0: My comment would be really great job of condensing a ton of Bitcoin tech and best practices that I've been recommending over the years into one explanation, including batching, consolidation, selection of inputs, using modern output types and some of the new tech like music, all into one verbal explanation, but also the write-up. So, applause for you for that. Gloria, anything to add before we move along?\n\n79\n01:12:22,707 --> 01:12:32,884\nSpeaker 5: Yeah. Great, great stuff. Next week is on fee estimation. So, feel free to tweet any questions you have about fee estimation. Try to answer them.\n\n80\n01:12:33,165 --> 01:13:44,119\nSpeaker 0: Next section from the newsletter is Q&A from the Bitcoin Stack Exchange. And so, every month we take an opportunity to pick out some of the most interesting questions and answers from the Stack Exchange and surface those in the newsletter. We have five for this week. The first one is testing pruning logic with Bitcoin D. And the person asking this question is attempting to do some testing around pruning and is wondering essentially the best way to do that. And conveniently, Lightlight points out the debug only and I think the hidden option, fast prune configuration option that uses smaller block files and a smaller minimum prune type specifically for testing the pruning logic. So, interesting configuration option that I hadn't seen before. that I thought might be interesting for folks. Next question is a question that Dave asked, which is governing motivation for the descendant size limit. And Suhas responded. And Dave, maybe since you asked the question and sort of had an angle here, maybe do you feel comfortable sort of summarizing an answer here?\n\n81\n01:13:47,082 --> 01:16:41,097\nSpeaker 3: Well, just the question was I was actually I'm doing an update on the book Mastering Bitcoin. And one of the things we wanted to add to the new edition of the book is stuff about CPFP feed bumping. And to do that, I also wanted to add a section about transaction pinning. And so, I was looking at the rules that end up with us having transaction pinning problems. And I was trying to figure out why exactly we have these rules. I'm sure it's documented somewhere, but and actually Suhas replied with some of the million list discussion where it is documented. But I was just sitting there trying to puzzle those out in my head and I said, hey, why not ask a question? And I got an answer from possibly the best person ever, especially with this subject. And so, Suhas's answer was that, like you summarized that we have two algorithms that are running simultaneously in Bitcoin, not simultaneously, but two reasons that we need to look at the amount of transactions that are related in the BEM pool at the same time. And the first one of those is pretty easy to understand. It's for helping miners find the best set of transactions to mine in a reasonable amount of time. And I think we discussed that quite a lot on, not we, but Gloria and Merch on last week's recap podcast and in last week's entry in the BEM pool session. So, anyone who's interested, go look at that. The other reason is eviction. So, since I think Bitcoin 10, 0.10, we've had a constant sized BEM pool or maximum sized BEM pool. So, the BEM pool can get to be 300 megabytes in size, which is about 150 megabytes of transactions, which in olden times used to be about one day worth of transactions. And we don't let it get any bigger than that, so the computer running our node doesn't run out of memory. Back in the day, it could run out of memory, which was bad. But if you're going to do that, that means you got to kick transactions out of the BEM pool every once in a while. And so, Suhas explains that we have these two algorithms and they're kind of the inverse of each other. We have an algorithm that tries to figure out which set of related transactions will provide us the best fee. And then the reverse algorithm, which says, which ones provide us the lowest fee, which ones are not worth keeping in the BEM pool. so we can kick those out and keep the most profitable ones in the BEM pool. And to do that in a reasonable amount of time, we have to limit the number of operations. Suhas explains that those algorithms can be quadratic in scaling. So, every time you double the amount of transactions they consider, you quadruple the amount of work. So, that's his answer, is that 25 is essentially more than 25. But the rules for 25 are reasonable limits within that quadratic scaling. And if that isn't a good answer, yeah, Gloria can maybe give more information if anybody is curious.\n\n82\n01:16:41,117 --> 01:16:42,944\nSpeaker 0: Anything to piggyback there, Gloria?\n\n83\n01:16:43,747 --> 01:16:57,219\nSpeaker 5: Oh, no, I was just laughing because I go through the same thing every time when someone asks me what the descendant limit is and I say it's 25. Well, it's 26, sometimes, if you have carve-out, but it's 25. That's why I was laughing.\n\n84\n01:16:57,720 --> 01:17:33,659\nSpeaker 0: Next question from the Stack Exchange is around running a bigger than default BEM pool. Folks may see that the default BEM pool has resulted in a full BEM pool lately with a lot of unconfirmed transactions out there and maybe an initial naive response may be, hey, I'm going to help the network. I'm going to increase the size of my BEM pool to be larger than the default so that I can accommodate all of these transactions that are outstanding. Rich, you asked and answered this question. Why might that not be a good idea?\n\n85\n01:17:34,700 --> 01:20:35,720\nSpeaker 1: Yeah, I don't want to say that it's not a good idea in that sense, but I think that people misunderstand when they think that it will benefit the network. So generally, our BEM pools are meant to make transactions available to anyone that wants to be a minor, and we want to make sure that especially all the juiciest transactions get to everyone. However, we have a bunch of other effects there that the BEM pool benefits with as well. So for example, we will cache all the validation of transactions that we have in our BEM pools, and we will relay transactions only when we see them for the first time. So every time a node learns about a new transaction, it will offer it to its peers. But if we have very different sizes in our BEM pools, people will keep around transactions longer than everyone else. So if they get rebroadcast, they will, for example, not relay them again because they still have them. So if someone offers me a transaction to my BEM pool that I already have, I will not request it for them, and I will also not announce it to all my peers again because I already have it. And on the other hand, if people mind blocks with transactions that nobody knows about anymore because everybody else dropped them from their BEM pools, then these blocks will propagate more slowly because we use a scheme called compact block relay, where we essentially only send the list of ingredients to a block, and everybody just rebuilds the block from their BEM pools if they have all the ingredients. But if transactions are missing, we will have extra round trips with asking back to the peer that sent us the block for those transactions. So we want our BEM pools to be as homogenous as possible. While making available all the best transactions to everyone, everybody should also drop the same things and enable people that currently don't signal replaceability yet to make replacements, and for those replacements to propagate smoothly, or if stuff needs to be rebroadcast because it fell out of most BEM pools, that it propagates smoothly. So it's sort of a misunderstanding when people think that running a bigger BEM pool will benefit people. They often think that their node will offer the same transactions again to other peers when they become relevant, but that's not the case. We do not ever rebroadcast transactions another time unless we learn about them afresh, or of course, if we include them into our block template, become nominated to be the author of the block by winning the distributed lottery, and then of course, packaging them into our block, then we would rebroadcast them as part of the block. Gloria, I probably missed something. Do you have comments?\n\n86\n01:20:37,840 --> 01:21:24,984\nSpeaker 5: Not particularly. I think maybe if you're a miner and you're interested in remembering transactions for longer, you wouldn't want to run your get block template on a huge BEM pool node. But yeah, I fully agree. The biggest thing was I think people thinking that they were going to rebroadcast the transactions that they were still keeping and therefore, that running a bigger one was better. But in fact, you become a black hole when they do rebroadcast because you won't re-download it and so you won't forward it again. So yeah, having a bigger BEM pool than average is not helping the network in any way. Just feel free to just keep it 300 megabytes.\n\n87\n01:21:27,711 --> 01:22:11,359\nSpeaker 0: Next question from the StackExchange is, what is the maximum number of inputs or outputs that a transaction can have? And this is actually a question that was asked in 2015. And the top answer currently is from Gavin Andreessen. And Murch, you provided an updated answer, which includes different calculations of inputs and outputs based on the SegWit and Taproot soft forks being activated. Folks can jump into your answer for the details about what would be the max number of inputs and what would be the max number of outputs. But I'm curious, how did you even come across this old question?\n\n88\n01:22:14,801 --> 01:23:54,012\nSpeaker 1: Somebody asked on Twitter because they were doing a podcast or listening to a podcast where that question came up. And I saw that we had that question already on StackExchange. But since, 2015 is a long time ago and we've had since activated the SegWit soft fork and the Taproot soft fork, there's a lot of new commonly used output types on the network that are more block space efficient. So while the limits are backwards compatible, or I should say forwards compatible. to old nodes that don't understand SegWit, we are actually able to have a lot more inputs and outputs on every transaction now, even though they are still within the same or forward compatible standard limits. So also the original answer did not include actual numbers. So I calculated that if we limit ourselves to commonly use payment types, so nothing fancy like up true pay to witness script hash inputs or up return outputs or stuff like that, we want to have a standard transaction that only uses either single SIG or multi-SIG constructions, then we would be able to have a transaction with slightly more than 3000 pay to witness public key hash outputs. And we could fashion a transaction with a little more than 1,700 inputs. So I think a lot of people might be surprised how many inputs and outputs we can have on transactions. And that's the context in which I think that is interesting. I don't think that we'll see a lot of transactions that actually approach those limits.\n\n89\n01:23:54,675 --> 01:24:13,811\nSpeaker 0: We just jinxed it. Next question from the Stack Exchange is, can a two of three multi-SIG, can the funds that are locked into a two of three multi-SIG be recovered without one of the X-Tubs? And folks may be familiar if you've done, if you use some tooling around software like\n\n90\n01:24:14,633 --> 01:24:15,374\nSpeaker 2: Spectre\n\n91\n01:24:15,695 --> 01:24:33,362\nSpeaker 0: or Sparrow, that part of the backup process is making sure that you have these output scripts in addition to backing up the keys themselves, because both are required to spend in the case of using, I guess, modern or common multi-SIG\n\n92\n01:24:33,883 --> 01:24:34,364\nSpeaker 2: outputs,\n\n93\n01:24:34,945 --> 01:25:08,039\nSpeaker 0: which would not include the bare multi-SIG output. I think you've answered almost all of the questions in the Stack Exchange this week. You recommended using an output script descriptor to back up the condition script. And you also noted, I think, that if there had been a previous spend using that same set of script conditions, that you could actually recover the funds, but otherwise that you would need a backup of all of the pub keys. Is that right?\n\n94\n01:25:10,621 --> 01:27:07,839\nSpeaker 1: Yes, but I think that the misunderstanding or concern that is the greatest in the context of multi-SIG is a lot of people misunderstand that when, for example, you have a two of three multi-SIG set up, you only need two of the public keys and two of the private keys, or rather just two of the private keys in order to spend your funds. But the problem with that is we use hash-based locks on our outputs that you have to prove that you know the original input script that the output creator had in mind, or rather the recipient had in mind when they requested the payment to that output. And so the thing is if you only have two private keys but don't know how the input script was constructed for which you need all three public keys, you will be unable to spend your funds. So a multi-SIG backup necessarily also has to keep all public keys. So the optimal way of backing up a multi-SIG, if you want to have it in distributed locations, would be, in my opinion, to have a backup of the construction of the input script that includes all of the necessary public keys and then one private key with each of the backups. So if you retain two of the backup shares, I should say, you will have all of the necessary private keys, but you also know how to construct the input script. So yeah, if you ever want to roll your own multi-SIG, I think that got a lot easier now that we have output script descriptors, because the output descriptor will include that information. And just be sure that you keep an output script descriptor with your private key backups if you do multi-SIG.\n\n95\n01:27:10,101 --> 01:27:26,454\nSpeaker 0: Next section of the newsletter is release and release candidates. We have one this week, which we touched on briefly last week, which is Bitcoin Core 25.0, which is a major release for Bitcoin Core. Gloria, I think we touched on last week, you mentioned the example\n\n96\n01:27:26,735 --> 01:27:26,855\nSpeaker 1: of\n\n97\n01:27:27,638 --> 01:27:43,019\nSpeaker 0: addressing the issue of a Raspberry Pi becoming a fire hazard, and I think that was related to, is that related to the blocks-only configuration memory fix, or was that one of the different fix unrelated to that?\n\n98\n01:27:49,602 --> 01:28:30,419\nSpeaker 1: Oh, I don't know. Gloria seems to have stepped away, maybe. Maybe I can take this one. So I think that we saw an unprecedented amount of transactions in the network recently, and the transaction submissions were very high. And we found that there were a few performance inefficiencies on nodes, and all three of the recent releases, the two-point releases, 23.2 and 24.1, as well as the new major release, 25.0, include a few performance improvements regarding just huge transaction loads. And I think that's what we were talking about last week as well.\n\n99\n01:28:32,744 --> 01:28:35,439\nSpeaker 0: Is there anything else that you'd like to highlight from the release merch?\n\n100\n01:28:48,141 --> 01:30:20,719\nSpeaker 1: Sorry, I don't have it in front of me right now. I was trying to pull it up, but I think that the main point is that Miniscript support is moving forward. You can now assign Miniscript transactions that use pay-to-witness script hash-based Miniscripts. I think in almost all cases, there's a small little thing that might not work yet. But some people are using that already to make more complex output script descriptors. For example, look at the recent announcement of Liana Wallet from Miami, which I think sounds very cool if you want to have built-in inheritance planning for all of your outputs while you're using your wallet. It's an open-source project, so I'm not getting paid to show this. And I think the other interesting thing was there is a new use case for the compact client-side block filters. So if you have a index for those, you may also know them as the L&D implementation, Neutrino nodes. Use these. You can now more quickly scan for wallets that you import by looking at your compact client-side block filters. You can think of compact client-side block filters as basically an index of what is included in every block. So by just having this table of content, you can way more easily skim what blocks you have to look at in detail in order to import a wallet, and that's implemented in this release now. I think that's the biggest, coolest new stuff.\n\n101\n01:30:22,901 --> 01:30:54,502\nSpeaker 0: We'll move on to the Notable Cone and Documentation Changes section of the newsletter, and at this point I'll solicit anybody who has a question or comment. You can raise your hand and request speaker access, and we'll try to get to your comment or question before we wrap up the newsletter. First PR this week is Bitcoin Core 27469, speeding up initial block download when using a wallet, and it sounds like there is a performance optimization that was implemented with this PR\n\n102\n01:30:54,883 --> 01:30:55,524\nSpeaker 1: such that\n\n103\n01:30:56,366 --> 01:31:07,395\nSpeaker 0: if you are using a wallet that has a known birthdate, meaning there were no transactions applicable to that wallet before that birthdate, that you can skip\n\n104\n01:31:08,177 --> 01:31:08,397\nSpeaker 4: some\n\n105\n01:31:08,798 --> 01:31:31,400\nSpeaker 0: scanning on blocks before that birthdate, and obviously that would free up some resources to be doing other things and not checking blocks. that would clearly not have any information about your wallet. Murch, I had a question for you on this one. Does this impact rescanning, or is this just during initial block download, if you're familiar?\n\n106\n01:31:33,280 --> 01:32:49,398\nSpeaker 1: Yeah, I was just scrolling through that a little bit. Of course, it specifies further down in the comments of that PR that it actually is only during IBD. The idea here is if you have a wallet loaded, then you not only process the block in order to build your chain state and your transaction index and what other indexes you have and catching up to the chain tip, you will also look at every block's content to see whether it's relevant to your wallet. However, with our wallet backups, or rather the wallet.dat file contains a birthdate. If it, for example, specifies that it was created only in 2020, there's absolutely no need to look at all the transactions before 2020 because we would never have created an address before that and never received any funds to the wallet if it's only been created in 2020. My understanding from a rudimentary glance at it is we are only skipping this extra lookups for wallet context during the IBD up to the time that our wallet actually was created, because we don't have to look for stuff that we could have never received anything in.\n\n107\n01:32:51,801 --> 01:33:40,559\nSpeaker 0: Next PR is Bitcoin Core 27626, parallel compact block downloads. And it sounds like that when if I'm running a node and I received notification of a new block in compact block format from a peer, I will then attempt to download any transactions that I need that I don't have for that block from that same peer. But there's potential that that peer is slow for whatever reason and not able to quickly reply. So in that case, we can use another node that has also already obtained the block and is more quickly able to give us the transactions that we're missing. Murch, did I get that right? Do you have anything to add?\n\n108\n01:33:41,260 --> 01:35:36,819\nSpeaker 1: Yeah. So basically the idea is this, when we use compact block relay, we're only transferring the table of contents of the block and the recipient will rebuild the block from the ingredients that they already have in the mempool. We touched upon this earlier today already. And if you are missing transactions, you still need to get those transactions. So basically you will have a short TXID that tells you, now include this transaction. And then you're like, wait, I don't have that in my mempool. Hey, note that I announced a block to me, give me that transaction. Now, if, for example, a block was received by one node first and they push it out to all of their 125 peers, hey, I have this new block. And everybody then asks for all of the missing transactions, because I don't know, somebody put some huge inscription into that block. Then they might be bandwidth constrained at that point to try to provide that data to all of their 125 peers. Well, 124, because they must have gotten the block from somewhere. But you know what I mean. So this new patch allows us to... Then the second node that maybe was the first one that got it from the first announcer, that also then announced that they now have the block. They basically announced with that too, that they have all the transactions that are in that block. So now instead of just waiting for the first node that announced a block to us and whose block header and compact block we accepted to also give us the transactions, we're going to go sideways and ask other nodes that also signal to us that they have the complete block to provide us the missing information. And this should help, especially with block relay, when there's a lot of transactions that are not readily available in everyone's mempools to propagate faster through the network.\n\n109\n01:35:37,020 --> 01:35:54,224\nSpeaker 0: Bitcoin Core 25 796, adding a new descriptor process PSBT RPC merch. Can you explain why we need another processing of PSBT RPC and how it interplays with some of the other RPC commands related to PSBTs? All right.\n\n110\n01:35:54,464 --> 01:37:27,339\nSpeaker 1: So PSBT stands for Partially Signed Bitcoin Transactions. We basically use that in the context of creating multi-user transactions. So for example, coin joins, or if you have multiple devices that need to sign off on stuff, or if you want to use UTXOs from multiple wallets that you own yourself, or if you're trying to sell an inscription by using PSBTs as the market announcement. And so PSBTs are super useful in making our transaction building easier for multiple parties together. And the way I understand descriptor process PSBT is, if you get a PSBT, you might need to fill in some of the blanks because you might not know about an input yet, or the person that said the wallet that created the PSBT might know what UTXOs they want to spend, but not have the whole UTXOs set to fill in all the blanks. So a node that has all that information might be asked to backfill the PSBT to provide all the relevant information. And with descriptor process PSBT, we can now express the missing data from descriptors. I'm bungling this a little bit, but basically it improves how we backfill the missing information and where we can look up that information to also include output descriptors, I think.\n\n111\n01:37:28,860 --> 01:38:11,139\nSpeaker 0: Next PR is from Eclair, Eclair 2668, which adds an upper bound on the fees paid during a force close operation. And the background here is that it doesn't really make sense to pay more fees than the amount that we may personally have at risk during a force close. So Eclair now computes the amount of funds that are at risk and compares that to some fee rate estimates and then acts accordingly. So you don't want to pay more in fees than you would be reclaiming. So there's a fix to optimize for that and do some heuristics in the calculation. So you don't do that anymore.\n\n112\n01:38:11,761 --> 01:39:32,102\nSpeaker 1: Right. So this would happen in the context, for example, when you are participating in a multi-hop payment and you accept it to create a remote HTLC, as in you lock in funds to forward the multi-hop payment, but then the fee rates on the mempool are going up immensely. And now when the payment times out, because the recipient doesn't pull it in, they're offline or something, we would have to go on chain in order to settle our channel in order not to lose those funds locked up in the HTLC. But of course, if we created the HTLC at a way lower fee rate environment, the HTLC itself might not be worth enough to go on chain for and force close and pay all this extra fee to process if the risk, say like 10 Satoshi payment is actually not worth it. So my understanding is that instead of force closing, they would just let that HTLC ride longer now. But yeah, hopefully then also they wouldn't create new HTLCs at that high fee rate that are that low value, but they wouldn't also go into force closing when there's too little value riding on it.\n\n113\n01:39:33,045 --> 01:40:46,208\nSpeaker 0: Next PR is also from Eclair. Eclair 2666, relaxing the reserve requirements on HTL receiver. And this is a mitigation for the stuck funds problem. And so in lightning, there's bolt two, which part of bolt two includes the channel reserve requirement. And that's recommended in the spec to be 1% of the channel total as reserve. And the idea is that each side of the channel would maintain that reserve so that there's some funds to lose if either party were to try to broadcast an old revoked commitment transaction. So if you didn't have this reserve, then there would be no risk of loss of funds by broadcasting all the transactions. And obviously when a channel is initially open, that reserve may not be met, but protocols attempts to work towards meeting that reserve. And so the interplay between the reserve requirement and the stuck funds problem is a bit unclear to me. So Murch, perhaps you can elaborate if you're familiar, otherwise we can try to see if Dave is still on.\n\n114\n01:40:46,890 --> 01:40:49,599\nSpeaker 1: I am not super familiar, but Dave raised his hand.\n\n115\n01:40:51,341 --> 01:42:07,903\nSpeaker 3: Yeah. So the issue here is that in a channel, you often get the case where most of the funds have moved to one side of the channel. So you had Alice and Bob, they have a channel together. And just through the national course of operations, Bob now has 99% of the funds and there's just 1% left for Alice. Now in the protocol, it's required that the receiver of the HTLC be responsible for paying its fees. But if Alice only has 1% left, now she can't receive any money from Bob. Even Bob wants to send her money, he can't because Alice is responsible for paying the fees, but the channel reserve requirement that 1% says that she has no money to spend. However, this doesn't actually make sense in this particular case, because if Bob tries to send money to Alice, he tries to push money. Well, if it succeeds, she's going to have the funds to pay the fees and if it fails, well, there's no fee required. So this just removes a requirement that's not actually a mitigation for any threat in this particular case. And there's also a related PR to update the specification for this and for some other things that can lead to fund stock problems.\n\n116\n01:42:08,665 --> 01:43:24,342\nSpeaker 0: Thanks for jumping in there, Dave. Next change is to BTC pay server, and this is the BIP-7E commit 97E7E, which begins setting the BIP-78 min fee rate parameter. So BTC pay server implements BIP-78 in terms of pay join features. So you can actually do pay join features if you're running BTC pay server. However, this parameter, this min fee rate was not being set. And the way that pay join works is there is some interactivity between sender and receiver who both contribute some inputs to the transaction. And if the second party contributes an input while not also increasing the fee amount to this min fee rate parameter, it could result in a transaction whose fee is below the sender's minimum fee rate amount. And so if you're not communicating this parameter during that interactivity, you could run into scenarios like this. So we noted a bug report. who that actually came from? a guest of ours who's working on pay join, who's Dan Gold, who was on with us last week. And he opened up this bug, which motivated the change to be more compliant with the spec.\n\n117\n01:43:25,144 --> 01:43:26,508\nSpeaker 2: Merch, thumbs up.\n\n118\n01:43:26,929 --> 01:44:32,039\nSpeaker 0: All right. And then the last PR for this week is to the BIPs repository. It is 1446, making a small change in a number of additions to the 340 on Schnorr signatures. And we note that these changes don't affect how the 340 is involved in consensus and signatures for taproot and tap script, but that it loosens the restriction that Schnorr signatures sign a message that is exactly 32 bytes. And I don't know the origin of the motivation for this relaxation of the signed message not needing to be 32 anymore. Merch, are you familiar with the motivation of why we don't need to sign exactly 32 bytes? I believe there was some discussion about some requirements on the implementations needing to hash something in order to get to those 32 bytes and that that was potentially an onerous thing. And being able to sign a message of any size was then the result of that, which resulted in this change.\n\n119\n01:44:32,681 --> 01:44:41,528\nSpeaker 1: I am not familiar. And this seems like a complex topic that I don't want to hazard getting into by just glancing at it too much.\n\n120\n01:44:41,548 --> 01:44:46,606\nSpeaker 0: Dave, do you have thoughts on that particular topic? Why was this particular change made?\n\n121\n01:44:48,813 --> 01:45:06,959\nSpeaker 3: The justification was just like you said, some programs might find it onerous to do this. You can implement, I think, but I believe you can, I don't know. I'm going to stick with Merch's claim, which is this is a complex topic. I'm qualified to talk about it. So I probably shouldn't. Sorry.\n\n122\n01:45:07,960 --> 01:45:29,250\nSpeaker 0: Okay. Well, if you're involved with Schnorr signatures and bit 340, you may want to look at the spec to see if there's some changes that affect you or optimizations that you can make as a result of change in the spec, but Bitcoin unaffected. I don't see any requests for speaker access or comments on our Twitter thread.\n\n123\n01:45:30,393 --> 01:46:04,403\nSpeaker 1: Maybe let me take a very attentive stab at it. It seems to be that if your message that you're trying to sign is already shorter than 32 bits, it would increase what you're signing and that doesn't necessarily make sense. So if you want to sign shorter messages, it might be better to directly sign the original instead of hashing it and blowing it up to 32 bytes. But this is again, just a very tentative read and yeah, maybe we'll pick some of the author's brands meanwhile and get back to this if it's important.\n\n124\n01:46:06,208 --> 01:46:37,234\nSpeaker 0: All right. We are just at the two hour mark, which makes this one of the longest, if not the longest podcast that we've done. I thank you all for joining us. I think we had a great discussion. Thanks to my cohost merch as always. Thanks to Gloria for joining us. Dave Harding for joining us. Yoast for joining us and Barack for joining us and providing their insights on their proposals and prototypes of things that they're working on. And we'll see you all next week for newsletter 254. Cheers.\n\n"}